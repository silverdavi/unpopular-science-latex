\begin{technical}
{\Large\textbf{Two Mathematical Realizations of Simpson’s Paradox}}\\[0.7em]
\vspace{1em}
\noindent\textbf{Reversal in Pearson Correlation}\\[0.5em]
Suppose two subgroups yield:
\begin{align*}
\operatorname{Corr}(X, Y \mid Z=1) &= +0.8, \\
\operatorname{Corr}(X, Y \mid Z=2) &= +0.7,
\end{align*}
yet the marginal correlation is:
\[
\operatorname{Corr}(X, Y) = -0.3.
\]

This reversal can occur when the subgroup means oppose each other:
\begin{align*}
\mathbb{E}[X \mid Z=1] &\ll \mathbb{E}[X \mid Z=2], \\
\mathbb{E}[Y \mid Z=1] &\gg \mathbb{E}[Y \mid Z=2].
\end{align*}

The total covariance decomposes as:
\begin{align}
\operatorname{Cov}(X, Y) &= \mathbb{E}[\operatorname{Cov}(X, Y \mid Z)] \notag \\
&\quad + \operatorname{Cov}(\mathbb{E}[X \mid Z], \mathbb{E}[Y \mid Z]).
\end{align}

The first term represents the true structural relationship. The second term arises from between-group mean shifts. When subgroup trends are consistent but means shift in opposite directions, this second term can dominate and flip the sign.

In such cases, the subgroup correlation reflects the actual relationship between the variables. The marginal correlation is an artifact of mixed distributions and should not be used to infer structure.

\noindent\textbf{Why Subgroup Correlations Reflect Structure}\\[0.5em]
The Pearson correlation coefficient assumes a homogeneous population. When data consist of subgroups (e.g., children vs. adults), the overall correlation reflects two effects:
\begin{itemize}
\item the correlation within each group,
\item the shift in means across groups.
\end{itemize}

This decomposes as:
\begin{align*}
\operatorname{Corr}(X, Y) &= 
\text{within-group structure} +\\& \quad\text{between-group shift}.
\end{align*}

If the subgroups differ in both \( \mathbb{E}[X \mid Z] \) and \( \mathbb{E}[Y \mid Z] \), the between-group term may dominate and flip the marginal sign — even if each group has a positive internal trend.

Subgroup correlations hold \( Z \) fixed and reveal how \( X \) relates to \( Y \) when background is controlled. The marginal correlation, in contrast, entangles structure with population imbalance.

For structural inference — e.g., how height relates to foot size, or how score relates to study time — \( \operatorname{Corr}(X, Y \mid Z) \) provides the interpretable relationship. The marginal \( \operatorname{Corr}(X, Y) \) may be distorted by mixing.

\vspace{0.5em}
\noindent\textit{Visual example:} Imagine both children and adults show that taller people have larger feet. But if children dominate the data and are both shorter and have smaller feet, the overall correlation may become negative. This reflects sample composition, not individual-level structure.

\vspace{1em}
\noindent\textbf{How Likely is Simpson’s Paradox?}\\[0.5em]
Pavlides and Perlman (2009) studied how often Simpson’s paradox arises in \( 2 \times 2 \times 2 \) contingency tables. Under a uniform distribution over all such tables, they showed:
\[
\boxed{\text{1 in 60 tables exhibits a reversal.}}
\]
This corresponds to a prior probability of \( \approx 0.0166 \) that conditional trends align while the aggregate trend opposes them.

The paradox becomes exponentially rarer with more subgroups. Under uniform assumptions, the chance of a reversal in three conditioning groups drops to roughly 0.0057.

\vspace{0.5em}
\noindent\textbf{References:}\\
Simpson, E. H. (1951). \textit{J. R. Stat. Soc. B}, 13(2), 238–241.\\
Pavlides, M. G., \& Perlman, M. D. (2009). \textit{J. Stat. Plan. Inference}, 139(1), 198–213.
\end{technical}
